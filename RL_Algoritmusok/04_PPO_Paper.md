# Proximal Policy Optimization Algorithms

# MUNKA ALATT, IDEIGLENES BECSEKKOL√ÅS!!


### John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov
### OpenAI
### {joschu, filip, prafulla, alec, oleg}@openai.com

## Abstract
A meger≈ës√≠t≈ë tanul√°shoz a politika gradiens m√≥dszereinek √∫j csal√°dj√°t javasoljuk, amelyek v√°ltakoznak az adatok mintav√©tele k√∂z√∂tt a k√∂rnyezettel val√≥ interakci√≥n kereszt√ºl, √©s a ‚Äûhelyettes√≠t≈ë‚Äù c√©lf√ºggv√©ny optimaliz√°l√°sa k√∂z√∂tt sztochasztikus gradiens emelked√©s seg√≠ts√©g√©vel. M√≠g a szabv√°nyos h√°zirend gradiens m√≥dszerek adatmint√°nk√©nt egy gradiens friss√≠t√©st hajtanak v√©gre, egy √∫j c√©lf√ºggv√©nyt javasolunk, amely lehet≈ëv√© teszi a minibatch friss√≠t√©sek t√∂bb korszak√°t. Az √∫j m√≥dszerek, amelyeket proxim√°lis politikaoptimaliz√°l√°snak (PPO) nevez√ºnk, rendelkeznek a bizalmi r√©gi√≥ politika optimaliz√°l√°s√°nak el≈ënyeivel.
tion (TRPO), de sokkal egyszer≈±bb a megval√≥s√≠t√°suk, √°ltal√°nosabbak √©s jobb a minta √∂sszetetts√©ge (empirikusan). K√≠s√©rleteink a PPO-t benchmark feladatok gy≈±jtem√©ny√©n tesztelik, bele√©rtve a szimul√°lt robotmozg√°st √©s az Atari-j√°t√©kokat, √©s megmutatjuk, hogy a PPO fel√ºlm√∫lja a t√∂bbi online ir√°nyelv gradiens m√≥dszer√©t, √©s √∂sszess√©g√©ben kedvez≈ë egyens√∫lyt tal√°l a minta √∂sszetetts√©ge, egyszer≈±s√©ge √©s a fali id≈ë k√∂z√∂tt.

## 1 Bevezet√©s
Az elm√∫lt √©vekben sz√°mos k√ºl√∂nb√∂z≈ë megk√∂zel√≠t√©st javasoltak a neur√°lis h√°l√≥zati f√ºggv√©ny k√∂zel√≠t≈ëkkel t√∂rt√©n≈ë meger≈ës√≠t≈ë tanul√°sra. A vezet≈ë versenyz≈ëk a m√©ly Q-learning [Mni+15], a ‚Äûvan√≠lia‚Äù politikai gradiens m√≥dszerek [Mni+16] √©s a bizalmi r√©gi√≥/term√©szetpolitikai gradiens m√≥dszerek [Sch+15b]. Van azonban m√©g mit jav√≠tani egy olyan m√≥dszer kifejleszt√©s√©ben, amely m√©retezhet≈ë (nagy modellekre √©s p√°rhuzamos implement√°ci√≥kra), adathat√©kony √©s robusztus (vagyis hiperparam√©terek hangol√°sa n√©lk√ºl is sikeres sz√°mos probl√©ma eset√©n). A Q-learning (f√ºggv√©nyk√∂zel√≠t√©ssel) sok egyszer≈± probl√©m√°ban kudarcot vall1, √©s kev√©ss√© √©rthet≈ë, a van√≠lia politikai gradiens m√≥dszerek gyenge adathat√©konys√°ggal √©s robusztuss√°ggal rendelkeznek; √©s a bizalmi r√©gi√≥ h√°zirend-optimaliz√°l√°sa (TRPO) viszonylag bonyolult, √©s nem kompatibilis azokkal az architekt√∫r√°kkal, amelyek zajt (p√©ld√°ul lemorzsol√≥d√°st) vagy param√©termegoszt√°st (a h√°zirend √©s az √©rt√©k f√ºggv√©ny k√∂z√∂tt, vagy seg√©dfeladatokat) tartalmaznak.
Ez a cikk a jelenlegi √°llapot jav√≠t√°s√°ra t√∂rekszik egy olyan algoritmus bevezet√©s√©vel, amely el√©ri a TRPO adathat√©konys√°g√°t √©s megb√≠zhat√≥ teljes√≠tm√©ny√©t, mik√∂zben csak els≈ërend≈± optimaliz√°l√°st alkalmaz.
√öj c√©lt javasolunk lev√°gott val√≥sz√≠n≈±s√©gi mutat√≥kkal, amely pesszimista becsl√©st (azaz als√≥ korl√°tot) k√©pez a politika teljes√≠tm√©ny√©re vonatkoz√≥an. A h√°zirendek optimaliz√°l√°sa √©rdek√©ben felv√°ltva mintav√©telezz√ºk a h√°zirendb≈ël sz√°rmaz√≥ adatokat, √©s t√∂bb optimaliz√°l√°si id≈ëszakot is v√©grehajtunk a mintav√©telezett adatokon.
K√≠s√©rleteink √∂sszehasonl√≠tj√°k a helyettes√≠t≈ë c√©l k√ºl√∂nb√∂z≈ë v√°ltozatainak teljes√≠tm√©ny√©t, √©s azt tal√°lt√°k, hogy a v√°gott val√≥sz√≠n≈±s√©gi ar√°nyokkal rendelkez≈ë verzi√≥ teljes√≠t a legjobban. √ñsszehasonl√≠tjuk a PPO-t sz√°mos kor√°bbi irodalmi algoritmussal is. A folyamatos vez√©rl√©si feladatokn√°l jobban teljes√≠t, mint az √°ltalunk √∂sszehasonl√≠that√≥ algoritmusok. Atari-n l√©nyegesen jobban teljes√≠t (a minta √∂sszetetts√©g√©t tekintve), mint az A2C, √©s hasonl√≥an az ACER-hez, b√°r sokkal egyszer≈±bb.

## 2 H√°tt√©r: A policy optimaliz√°l√°sa
### 2.1. Szab√°lyzati gradiens m√≥dszerek
A policy gradiens m√≥dszerek √∫gy m≈±k√∂dnek, hogy kisz√°m√≠tj√°k a policy gradiens becsl√©s√©t, √©s egy sztochasztikus gradiens emelked√©si algoritmushoz csatlakoztatj√°k. A leggyakrabban haszn√°lt gradiensbecsl≈ë a k√∂vetkez≈ëvel rendelkezik:

(1)

ahol œÄŒ∏ egy sztochasztikus politika √©s ÀÜAt az el≈ënyf√ºggv√©ny becsl√©se a t id≈ëpontban.
Itt az elv√°r√°s 


az empirikus √°tlagot jelzi egy v√©ges mintak√∂tegre egy olyan algoritmusban, amely a mintav√©tel √©s az optimaliz√°l√°s k√∂z√∂tt v√°ltakozik. Automatikus alkalmaz√°st haszn√°l√≥ megval√≥s√≠t√°sok
a differenci√°l√≥ szoftver olyan c√©lf√ºggv√©ny fel√©p√≠t√©s√©vel m≈±k√∂dik, amelynek gradiense a politikai gradiens becsl√©se; a ÀÜg becsl≈ët a c√©l differenci√°l√°s√°val kapjuk
(2)
B√°r vonz√≥ t√∂bb optimaliz√°l√°si l√©p√©st v√©grehajtani ezen a vesztes√©ges LP G-n ugyanazon a p√°ly√°n, ez nem kell≈ëen indokolt, √©s empirikusan gyakran puszt√≠t√≥an nagy h√°zirend-friss√≠t√©sekhez vezet (l√°sd a 6.1. szakaszt; az eredm√©nyek nem jelennek meg, de hasonl√≥ak voltak vagy rosszabb, mint a ‚Äûnincs kiv√°g√°s vagy b√ºntet√©s‚Äù be√°ll√≠t√°s).

### 2.2 Bizalmi r√©gi√≥ met√≥dusok
A TRPO-ban [Sch+15b] egy c√©lf√ºggv√©ny (a ‚Äûhelyettes√≠t≈ë‚Äù c√©l) maximaliz√°l√°sra ker√ºl, a politikafriss√≠t√©s m√©ret√©re vonatkoz√≥ korl√°toz√°s f√ºggv√©ny√©ben. Pontosabban maximaliz√°lni

(3)
(4)
Itt a Œ∏old a h√°zirend-param√©terek vektora a friss√≠t√©s el≈ëtt. Ez a probl√©ma hat√©konyan k√∂zel√≠t≈ëleg megoldhat√≥ a konjug√°lt gradiens algoritmussal, miut√°n line√°ris k√∂zel√≠t√©st v√©gz√ºnk a c√©lhoz √©s m√°sodfok√∫ k√∂zel√≠t√©st a k√©nyszerhez.
A TRPO-t igazol√≥ elm√©let val√≥j√°ban egy b√ºntet√©s alkalmaz√°s√°t javasolja megszor√≠t√°s helyett, azaz a korl√°tlan optimaliz√°l√°si probl√©ma megold√°s√°t


valamilyen Œ≤ egy√ºtthat√≥ra. Ez abb√≥l a t√©nyb≈ël k√∂vetkezik, hogy egy bizonyos helyettes√≠t≈ë c√©l (amely kisz√°m√≠tja
a max. KL over states az √°tlag helyett) als√≥ korl√°tot (azaz pesszimista korl√°tot) k√©pez a
a politika teljes√≠tm√©nye œÄ. A TRPO kem√©ny k√©nyszert alkalmaz, nem pedig b√ºntet√©st, mert kem√©ny
hogy egyetlen Œ≤-√©rt√©ket v√°lasszunk, amely j√≥l teljes√≠t a k√ºl√∂nb√∂z≈ë probl√©m√°k k√∂z√∂tt ‚Äì vagy ak√°r egyetlenegyen bel√ºl is
probl√©ma, ahol a jellemz≈ëk a tanul√°s sor√°n v√°ltoznak. Ez√©rt a c√©lunk el√©r√©se √©rdek√©ben
egy els≈ërend≈± algoritmus, amely a TRPO monoton jav√≠t√°s√°t emul√°lja, a k√≠s√©rletek azt mutatj√°k
hogy nem elegend≈ë egyszer≈±en egy r√∂gz√≠tett Œ≤ b√ºntet√©si egy√ºtthat√≥t v√°lasztani √©s optimaliz√°lni a b√ºntetett
objekt√≠v egyenlet (5) SGD-vel; tov√°bbi m√≥dos√≠t√°sok sz√ºks√©gesek.



valamilyen Œ≤ egy√ºtthat√≥ra. Ez abb√≥l a t√©nyb≈ël k√∂vetkezik, hogy egy bizonyos helyettes√≠t≈ë c√©l (amely az √°tlag helyett a max. KL-t sz√°molja ki az √°llapotok felett) als√≥ korl√°tot (azaz pesszimista korl√°tot) k√©pez a œÄ politika teljes√≠tm√©ny√©re. A TRPO kem√©ny megszor√≠t√°st alkalmaz, nem pedig b√ºntet√©st, mert neh√©z egyetlen Œ≤-√©rt√©ket kiv√°lasztani, amely j√≥l teljes√≠t a k√ºl√∂nb√∂z≈ë probl√©m√°k k√∂z√∂tt ‚Äì vagy ak√°r egyetlen probl√©m√°n bel√ºl is, ahol a jellemz≈ëk a tanul√°s sor√°n v√°ltoznak. Ez√©rt a TRPO monoton jav√≠t√°s√°t emul√°l√≥ els≈ërend≈± algoritmus c√©lj√°nak el√©r√©s√©hez a k√≠s√©rletek azt mutatj√°k, hogy nem elegend≈ë egyszer≈±en egy r√∂gz√≠tett Œ≤ b√ºntet√©si egy√ºtthat√≥t v√°lasztani √©s optimaliz√°lni a b√ºntetett √©rt√©ket.
objekt√≠v egyenlet (5) SGD-vel; tov√°bbi m√≥dos√≠t√°sok sz√ºks√©gesek.


## 3 Kiv√°gott Surrogate Objective
Jel√∂lje rt(Œ∏) a val√≥sz√≠n≈±s√©gi ar√°nyt rt(Œ∏) = œÄŒ∏ (at | st)
œÄŒ∏old (at | st) , teh√°t r(Œ∏old) = 1. A TRPO maximaliz√°lja a ‚Äûhelyettes√≠t≈ë‚Äù c√©lt

(6)
A CP I fels≈ë index a konzervat√≠v politikai iter√°ci√≥ra utal [KL02], ahol ezt a c√©lt javasolt√°k. Korl√°toz√°s n√©lk√ºl az LCP I maximaliz√°l√°sa t√∫lzottan nagy m√©rt√©k≈± szab√°lyzatfriss√≠t√©shez vezetne; ez√©rt most megfontoljuk, hogyan m√≥dos√≠tsuk a c√©lt, hogy szankcion√°ljuk a szab√°lyzat azon v√°ltoztat√°sait, amelyek az rt(Œ∏)-t 1-r≈ël elt√°vol√≠tj√°k.

Az √°ltalunk javasolt f≈ë c√©l a k√∂vetkez≈ë:

ahol az epsilon egy hiperparam√©ter, mondjuk = 0,2. E c√©lkit≈±z√©s motiv√°ci√≥ja a k√∂vetkez≈ë. A min bel√ºl az els≈ë tag az LCP I . A m√°sodik tag, a clip(rt(Œ∏), 1 ‚àí , 1 + ) ÀÜAt, m√≥dos√≠tja a helyettes√≠t≈ë c√©lt a val√≥sz√≠n≈±s√©gi ar√°ny lev√°g√°s√°val, ami elt√°vol√≠tja az √∂szt√∂nz√©st arra, hogy rt az [1 ‚àí , 1 + ] intervallumon k√≠v√ºlre helyezze. V√©g√ºl vessz√ºk a v√°gott √©s a le nem v√°gott objekt√≠v minimum√°t, √≠gy a v√©gs≈ë c√©l a lev√°gatlan objekt√≠v als√≥ korl√°tja (azaz egy pesszimista korl√°t). Ezzel a s√©m√°val csak akkor hagyjuk figyelmen k√≠v√ºl a val√≥sz√≠n≈±s√©gi ar√°ny v√°ltoz√°s√°t, ha az a c√©lt jav√≠tan√°, √©s akkor vessz√ºk figyelembe, ha rontja a c√©lt. Figyelj√ºk meg, hogy az LCLIP (Œ∏) = LCP I (Œ∏) a Œ∏old k√∂r√ºli els≈ë sorrendben (azaz ahol r = 1), azonban elt√©r≈ëekk√© v√°lnak, ahogy Œ∏ elt√°volodik a Œ∏oldt√≥l.

1. √°bra
egyetlen tagot (azaz egyetlen t-t) √°br√°zol az LCLIP-ben; vegye figyelembe, hogy az r val√≥sz√≠n≈±s√©gi ar√°nyt 1 ‚àí vagy 1 + √©rt√©kre v√°gjuk, att√≥l f√ºgg≈ëen, hogy az el≈ëny pozit√≠v vagy negat√≠v.r
LCLIP
0 1 1+
A > 0r
LCLIP
0 11 ‚àí
A < 0

1. √°bra: Az LCLIP helyettes√≠t≈ë f√ºggv√©ny egy tagj√°t (azaz egyetlen id≈ël√©p√©st) √°br√°zol√≥ diagramok az r val√≥sz√≠n≈±s√©gi ar√°ny f√ºggv√©ny√©ben, pozit√≠v el≈ëny√∂k (balra) √©s negat√≠v el≈ëny√∂k (jobb oldala) eset√©n. Az egyes diagramokon l√©v≈ë piros k√∂r az optimaliz√°l√°s kiindul√≥pontj√°t mutatja, azaz r = 1. Vegye figyelembe, hogy az LCLIP ezek k√∂z√ºl a kifejez√©sek k√∂z√ºl sokat √∂sszegez.

A 2. √°bra egy m√°sik intu√≠ci√≥s forr√°st ad az LCLIP helyettes√≠t≈ë objekt√≠vvel kapcsolatban. Megmutatja, hogy sz√°mos c√©lkit≈±z√©s hogyan v√°ltozik, ahogyan a h√°zirend-friss√≠t√©si ir√°ny ment√©n interpol√°lunk, amelyet a proxim√°lis h√°zirend-optimaliz√°l√°ssal (az algoritmus, amelyet hamarosan bemutatunk) kapunk egy folyamatos ellen≈ërz√©si probl√©m√°ra. L√°thatjuk, hogy az LCLIP az LCP I als√≥ korl√°tja, √©s b√ºntet√©s j√°r a t√∫l nagy h√°zirend-friss√≠t√©s√©rt.

2. √°bra: Helyettes√≠t≈ë c√©lok, amikor interpol√°lunk a kezdeti Œ∏old h√°zirend-param√©ter √©s a friss√≠tett h√°zirend-param√©ter k√∂z√∂tt, amelyet a PPO egy iter√°ci√≥ja ut√°n sz√°m√≠tunk ki. A friss√≠tett h√°zirend KL elt√©r√©se k√∂r√ºlbel√ºl 0,02 a kezdeti h√°zirendhez k√©pest, √©s ez az a pont, ahol az LCLIP maxim√°lis. Ez a diagram megfelel a Hopper-v1 probl√©ma els≈ë h√°zirend-friss√≠t√©s√©nek, a 6.1. szakaszban megadott hiperparam√©terek haszn√°lat√°val.

## 4 Adapt√≠v KL b√ºntet√©si egy√ºtthat√≥
Egy m√°sik megk√∂zel√≠t√©s, amely a kiv√°gott helyettes√≠t≈ë c√©lkit≈±z√©s alternat√≠v√°jak√©nt, vagy mellette haszn√°lhat√≥, az, hogy a KL divergenci√°ra b√ºntet√©st alkalmazunk, √©s a b√ºntet√©si egy√ºtthat√≥t √∫gy adapt√°ljuk, hogy el√©rj√ºk a KL divergencia dtarg valamilyen c√©l√©rt√©k√©t. politika friss√≠t√©se. K√≠s√©rleteink sor√°n azt tal√°ltuk, hogy a KL-b√ºntet√©s rosszabbul teljes√≠tett, mint a kiv√°gott helyettes√≠t≈ë objekt√≠v, azonban ide soroltuk, mert ez egy fontos alap√©rt√©k.
Ennek az algoritmusnak a legegyszer≈±bb p√©ld√°ny√°ban a k√∂vetkez≈ë l√©p√©seket hajtjuk v√©gre minden h√°zirend-friss√≠t√©sn√©l:
‚Ä¢ T√∂bb korszaknyi minibatch SGD haszn√°lat√°val optimaliz√°lja a KL-b√ºntetett objekt√≠vet

(8)
‚Ä¢ Sz√°m√≠tsa ki d = ÀÜEt[KL[œÄŒ∏old (¬∑ | st), œÄŒ∏(¬∑ | st)]]
‚Äì Ha d < dtarg/1,5, Œ≤ ‚Üê Œ≤/2
‚Äì Ha d > dtarg √ó 1,5, Œ≤ ‚Üê Œ≤ √ó 2
A friss√≠tett Œ≤-t a rendszer a k√∂vetkez≈ë h√°zirend-friss√≠t√©shez haszn√°lja. Ezzel a s√©m√°val id≈ënk√©nt l√°thatunk olyan h√°zirend-friss√≠t√©seket, ahol a KL elt√©r√©s jelent≈ësen elt√©r a dtarg-t√≥l, azonban ezek ritk√°k, √©s a Œ≤ gyorsan alkalmazkodik. A fenti 1.5 √©s 2 param√©tereket heurisztikusan v√°lasztottuk, de az algoritmus nem t√∫l √©rz√©keny r√°juk. A Œ≤ kezdeti √©rt√©ke egy m√°sik hiperparam√©ter, de a gyakorlatban nem fontos, mert az algoritmus gyorsan m√≥dos√≠tja.

## 5 Algoritmus
Az el≈ëz≈ë szakaszok helyettes√≠t≈ë vesztes√©gei kisz√°m√≠that√≥k √©s megk√ºl√∂nb√∂ztethet≈ëk egy tipikus politikai gradiens implement√°ci√≥ kisebb v√°ltoztat√°s√°val. Az automatikus differenci√°l√°st haszn√°l√≥ megval√≥s√≠t√°sokn√°l egyszer≈±en meg kell alkotni az LCLIP vagy LKLP EN vesztes√©get az LP G helyett, √©s t√∂bb l√©p√©sben sztochasztikus gradiens emelked√©st kell v√©grehajtani ezen a c√©lon.

A legt√∂bb varianciacs√∂kkentett el≈ëny-f√ºggv√©ny becsl≈ë sz√°m√≠t√°si technik√°ja V (s) tanult √°llapot-√©rt√©k f√ºggv√©nyt haszn√°l; p√©ld√°ul az √°ltal√°nos√≠tott el≈ënybecsl√©s [Sch+15a], vagy a v√©ges horizont√∫ becsl√©sek [Mni+16]-ban. Ha olyan neur√°lis h√°l√≥zati architekt√∫r√°t haszn√°lunk, amely megosztja a param√©tereket a h√°zirend- √©s az √©rt√©kf√ºggv√©ny k√∂z√∂tt, akkor olyan vesztes√©gf√ºggv√©nyt kell haszn√°lnunk, amely egyes√≠ti a h√°zirend helyettes√≠t≈ëj√©t √©s az √©rt√©kf√ºggv√©ny hibatagj√°t. Ezt a c√©lt tov√°bb lehet n√∂velni egy entr√≥pia b√≥nusz hozz√°ad√°s√°val az elegend≈ë felt√°r√°s biztos√≠t√°s√°ra, amint azt a kor√°bbi munk√°kban javasolt√°k [Wil92; Mni+16].
Ezeket a kifejez√©seket kombin√°lva a k√∂vetkez≈ë c√©lt kapjuk, amely (hozz√°vet≈ëlegesen) maximaliz√°lt
minden iter√°ci√≥:

 (9)
ahol c1, c2 egy√ºtthat√≥k, S pedig entr√≥piab√≥nuszt, L VFt pedig n√©gyzetes hibavesztes√©get (VŒ∏(st) ‚àí Vtargt)2.

Az [Mni+16]-ban n√©pszer≈±s√≠tett √©s ism√©tl≈ëd≈ë neur√°lis h√°l√≥zatokhoz j√≥l haszn√°lhat√≥ ir√°nyelv-gradiens megval√≥s√≠t√°si st√≠lus T id≈ël√©p√©sre futtatja a h√°zirendet (ahol T sokkal kisebb, mint az epiz√≥d hossza), √©s az √∂sszegy≈±jt√∂tt mint√°kat egy friss√≠t√©s. Ez a st√≠lus olyan el≈ënybecsl≈ët ig√©nyel, amely nem n√©z t√∫l a T id≈ël√©p√©sen. Az [Mni+16] √°ltal haszn√°lt becsl√©s
 (10)
ahol t adja meg az id≈ëindexet [0, T]-ben, egy adott T hossz√∫s√°g√∫ p√°lyaszakaszon bel√ºl. Ezt a v√°laszt√°st √°ltal√°nos√≠tva haszn√°lhatjuk az √°ltal√°nos√≠tott el≈ënybecsl√©s csonka v√°ltozat√°t, amely a (10) egyenletre reduk√°l√≥dik, ha Œª = 1:


Az al√°bbiakban l√°that√≥ egy proxim√°lis h√°zirend-optimaliz√°l√°si (PPO) algoritmus, amely r√∂gz√≠tett hossz√∫s√°g√∫ p√°lyaszegmenseket haszn√°l. Minden iter√°ci√≥, N (p√°rhuzamos) szerepl≈ë mindegyike T id≈ël√©p√©snyi adatot gy≈±jt. Ezut√°n megszerkesztj√ºk a helyettes√≠t≈ë vesztes√©get ezeken az NT id≈ël√©p√©seken, √©s optimaliz√°ljuk a minibatch SGD-vel (vagy √°ltal√°ban a jobb teljes√≠tm√©ny √©rdek√©ben Adam [KB14]) K epoch√°kra.

## 6 K√≠s√©rlet
### 6.1 A helyettes√≠t≈ë c√©lok √∂sszehasonl√≠t√°sa
El≈ësz√∂r is √∂sszehasonl√≠tunk t√∂bb k√ºl√∂nb√∂z≈ë helyettes√≠t≈ë c√©lt k√ºl√∂nb√∂z≈ë hiperparam√©terek alatt. Itt √∂sszehasonl√≠tjuk az LCLIP helyettes√≠t≈ë objekt√≠vet sz√°mos term√©szetes vari√°ci√≥val √©s abl√°lt v√°ltozattal.


A KL-b√ºntet√©shez haszn√°lhatunk r√∂gz√≠tett Œ≤ b√ºntet√©si egy√ºtthat√≥t vagy adapt√≠v egy√ºtthat√≥t a 4. szakaszban le√≠rtak szerint a dtarg KL c√©l√©rt√©k haszn√°lat√°val. Ne feledje, hogy a napl√≥z√≥n√°ban is pr√≥b√°ltunk v√°gni, de a teljes√≠tm√©ny nem volt jobb.
Mivel az egyes algoritmusv√°ltozatokhoz hiperparam√©tereket keres√ºnk, egy sz√°m√≠t√°si szempontb√≥l olcs√≥ benchmarkot v√°lasztottunk az algoritmusok tesztel√©s√©hez. Nevezetesen 7 db szimul√°lt robotikai feladatot2 alkalmaztunk az OpenAI Gymben [Bro+16], melyek a MuJoCo [TET12] fizikai motort haszn√°lj√°k. Mindegyiken egymilli√≥ l√©p√©snyi edz√©st v√©gz√ºnk. A kiv√°g√°shoz haszn√°lt hiperparam√©terek ( ) √©s a KL b√ºntet√©s (Œ≤, dtarg) mellett, amelyekre keres√ºnk, a t√∂bbi hiperparam√©tert a 3. t√°bl√°zat tartalmazza.
A h√°zirend √°br√°zol√°s√°hoz egy teljesen √∂sszekapcsolt MLP-t haszn√°ltunk k√©t, 64 egys√©gb≈ël √°ll√≥ rejtett r√©teggel √©s tanh nemlinearit√°sokkal, a Gauss-eloszl√°s √°tlag√°t adva ki v√°ltoz√≥ sz√≥r√°ssal, k√∂vetve [Sch+15b; Dua+16]. Nem osztjuk meg a param√©tereket a h√°zirend √©s az √©rt√©kf√ºggv√©ny k√∂z√∂tt (teh√°t a c1 egy√ºtthat√≥ irrelev√°ns), √©s nem haszn√°lunk entr√≥pia b√≥nuszt.
Mindegyik algoritmus mind a 7 k√∂rnyezetben futott, mindegyiken 3 v√©letlenszer≈± maggal. Az algoritmus minden egyes futtat√°s√°t √∫gy √©rt√©kelt√ºk, hogy kisz√°m√≠tottuk az utols√≥ 100 epiz√≥d √°tlagos teljes jutalm√°t. Az egyes k√∂rnyezetekhez tartoz√≥ pontsz√°mokat eltoltuk √©s sk√°l√°ztuk √∫gy, hogy a v√©letlenszer≈± h√°zirend 0-t adjon, a legjobb eredm√©nyt pedig 1-re √°ll√≠tottuk, √©s 21 fut√°sb√≥l √°tlagoltuk, hogy minden algoritmusbe√°ll√≠t√°shoz egyetlen skal√°rt kapjunk.
Az eredm√©nyeket az 1. t√°bl√°zat mutatja. Megjegyzend≈ë, hogy a pontsz√°m negat√≠v a v√°g√°s vagy b√ºntet√©s n√©lk√ºli be√°ll√≠t√°sn√°l, mivel egy k√∂rnyezet (f√©l gep√°rd) eset√©n nagyon negat√≠v pontsz√°mot ad, ami rosszabb, mint a kezdeti v√©letlenszer≈± h√°zirend.

| algorithm avg. | normalized score |
| --- | --- |
| No clipping or penalty | -0.39 |
| Clipping,  = 0.1 | 0.76 |
| Clipping,  = 0.2 | 0.82 |
| Clipping,  = 0.3 | 0.70 |
| Adaptive KL dtarg = 0.003 | 0.68 |
| Adaptive KL dtarg = 0.01 | 0.74 |
| Adaptive KL dtarg = 0.03 | 0.71 |
| Fixed KL, Œ≤ = 0.3 | 0.62 |
| Fixed KL, Œ≤ = 1. | 0.71 |
| Fixed KL, Œ≤ = 3. | 0.72 |
| Fixed KL, Œ≤ = 10. | 0.69 |

1. t√°bl√°zat: A folyamatos ellen≈ërz√©s benchmark eredm√©nyei. √Åtlagos normaliz√°lt pontsz√°mok (az algoritmus t√∂bb mint 21 futtat√°sa, 7 k√∂rnyezetben) minden algoritmus/hiperparam√©ter-be√°ll√≠t√°shoz. Œ≤-t 1-re inicializ√°ltuk.

## 6.2 √ñsszehasonl√≠t√°s a folyamatos tartom√°ny m√°s algoritmusaival
Ezut√°n √∂sszehasonl√≠tjuk a PPO-t (a 3. szakasz ‚Äûkiv√°gott‚Äù helyettes√≠t≈ë c√©lkit≈±z√©s√©vel) sz√°mos m√°s szakirodalmi m√≥dszerrel, amelyeket folyamatos probl√©m√°k eset√©n hat√©konynak tartanak. √ñsszehasonl√≠tottuk a k√∂vetkez≈ë algoritmusok hangolt implement√°ci√≥ival: bizalmi r√©gi√≥ politika optimaliz√°l√°sa [Sch+15b], keresztentr√≥pia m√≥dszer (CEM) [SL06], van√≠lia politika gradiens adapt√≠v l√©p√©sm√©rettel3, A2C [Mni+16], A2C bizalmi r√©gi√≥val [ Wan+16]. Az A2C az el≈ëny aktor kritikus r√∂vid√≠t√©se, √©s az A3C szinkron v√°ltozata, amelyr≈ël azt tal√°ltuk, hogy ugyanolyan vagy jobb teljes√≠tm√©nyt ny√∫jt, mint az aszinkron verzi√≥. A PPO-hoz az el≈ëz≈ë szakasz hiperparam√©tereit haszn√°ltuk, ahol = 0,2. Azt l√°tjuk, hogy a PPO szinte minden folyamatos vez√©rl√©si k√∂rnyezetben fel√ºlm√∫lja az el≈ëz≈ë m√≥dszereket.

3. √°bra: T√∂bb algoritmus √∂sszehasonl√≠t√°sa t√∂bb MuJoCo k√∂rnyezetben, egymilli√≥ id≈ël√©p√©sre val√≥ betan√≠t√°s.

## 6.3 Bemutat√≥ a folyamatos tartom√°nyban: Humanoid fut√°s √©s korm√°nyz√°s
Annak √©rdek√©ben, hogy bemutassuk a PPO teljes√≠tm√©ny√©t a nagy dimenzi√≥s folyamatos vez√©rl√©si probl√©m√°kkal kapcsolatban, egy 3D-s humanoidot mag√°ban foglal√≥ feladatsoron edz√ºnk, ahol a robotnak futnia, korm√°nyoznia kell, √©s fel kell kelnie a f√∂ldr≈ël, esetleg mik√∂zben kock√°k dob√°lj√°k. Az √°ltalunk tesztelt h√°rom feladat a k√∂vetkez≈ë: (1) RoboschoolHumanoid: csak el≈ërefel√© t√∂rt√©n≈ë mozg√°s, (2) RoboschoolHumanoidFlagrun: a c√©lpont helyzete v√©letlenszer≈±en v√°ltozik 200 l√©p√©senk√©nt, vagy amikor el√©rj√ºk a c√©lt, (3) RoboschoolHumanoidFlagrunHarder, ahol a robotot kock√°kkal dob√°lj√°k √©s fel kell kelnie a f√∂ldr≈ël. L√°sd az 5. √°br√°t egy tanult ir√°nyelv √°ll√≥k√©peinek megjelen√≠t√©s√©hez, √©s a 4. √°br√°n a h√°rom feladat tanul√°si g√∂rb√©i√©rt. A hiperparam√©tereket a 4. t√°bl√°zat tartalmazza. Egyidej≈± munk√°ban Heess et al. [Hee+17] a PPO adapt√≠v KL-v√°ltozat√°t (4. szakasz) haszn√°lta a 3D-s robotok helyv√°ltoztat√°si ir√°nyelveinek megismer√©s√©re.

5. √°bra: A RoboschoolHumanoidFlagrun-t√≥l tanult h√°zirend √°ll√≥k√©p-keretei. Az els≈ë hat k√©pkock√°ban a robot egy c√©l fel√© fut. Ezut√°n a poz√≠ci√≥ v√©letlenszer≈±en megv√°ltozik, √©s a robot megfordul, √©s az √∫j c√©l fel√© fut.

### 6.4 √ñsszehasonl√≠t√°s m√°s algoritmusokkal az Atari tartom√°nyban
A PPO-t az Arcade Learning Environment [Bel+15] benchmarkon is futtattuk, √©s √∂sszehasonl√≠tottuk az A2C [Mni+16] √©s az ACER [Wan+16] j√≥l hangolt implement√°ci√≥ival. Mindh√°rom algoritmus eset√©ben ugyanazt a h√°zirend-h√°l√≥zati architekt√∫r√°t haszn√°ltuk, mint az [Mni+16]-ban. A PPO hiperparam√©tereit az 5. t√°bl√°zat tartalmazza. A m√°sik k√©t algoritmushoz olyan hiperparam√©tereket haszn√°ltunk, amelyeket √∫gy hangoltunk, hogy maximaliz√°ljuk a teljes√≠tm√©nyt ezen a referencia√©rt√©ken.
Az eredm√©nyek √©s a tanul√°si g√∂rb√©k t√°bl√°zata mind a 49 j√°t√©kra vonatkoz√≥an a B. f√ºggel√©kben tal√°lhat√≥. A k√∂vetkez≈ë k√©t pontoz√°si mutat√≥t vessz√ºk figyelembe: (1) epiz√≥donk√©nti √°tlagos jutalom a teljes edz√©si id≈ëszak alatt (ami a gyors tanul√°st seg√≠ti el≈ë), √©s (2) √°tlagos jutalom per epiz√≥d. epiz√≥d az edz√©s utols√≥ 100 epiz√≥dj√°b√≥l (ami kedvez a v√©gs≈ë teljes√≠tm√©nynek). A 2. t√°bl√°zat az egyes algoritmusok √°ltal ‚Äûnyert‚Äù j√°t√©kok sz√°m√°t mutatja, ahol a gy≈ëztest √∫gy sz√°m√≠tjuk ki, hogy a h√°rom pr√≥ba pontoz√°si mutat√≥j√°t √°tlagoljuk.

| | A2C | ACER | PPO | Tie |
| --- | --- | --- | --- | --- |
| (1) avg. episode reward over all of training | 1 | 18 | 30 | 0 |
| (2) avg. episode reward over last 100 episodes | 1 | 28 | 19 | 1 |
2. t√°bl√°zat: Az egyes algoritmusok √°ltal ‚Äûnyert‚Äù j√°t√©kok sz√°ma, ahol a pontoz√°si mutat√≥t h√°rom pr√≥ba √°tlaga alapj√°n sz√°m√≠tj√°k ki.

## 7 K√∂vetkeztet√©s
Bevezett√ºk a proxim√°lis h√°zirend-optimaliz√°l√°st, a h√°zirend-optimaliz√°l√°si m√≥dszerek egy olyan csal√°dj√°t, amely t√∂bb sztochasztikus gradiens emelked√©st haszn√°l az egyes szab√°lyzatfriss√≠t√©sek v√©grehajt√°s√°hoz. Ezek a m√≥dszerek a bizalom-r√©gi√≥ m√≥dszerek stabilit√°s√°val √©s megb√≠zhat√≥s√°g√°val rendelkeznek, de sokkal egyszer≈±bb a megval√≥s√≠t√°suk, mind√∂ssze n√©h√°ny sornyi k√≥dv√°lt√°st ig√©nyelnek a van√≠lia h√°zirend gradiens megval√≥s√≠t√°s√°hoz, amely √°ltal√°nosabb be√°ll√≠t√°sokban alkalmazhat√≥ (p√©ld√°ul k√∂z√∂s architekt√∫ra haszn√°latakor a h√°zirendhez √©s √©rt√©kf√ºggv√©ny), √©s jobb √°ltal√°nos teljes√≠tm√©nyt ny√∫jtanak.

## 8 K√∂sz√∂netnyilv√°n√≠t√°s
K√∂sz√∂net Rocky Duannak, Peter Chennek √©s m√°soknak az OpenAI-n√°l az √©lesl√°t√≥ megjegyz√©sek√©rt.

## References
<pre>[Bel+15] M. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. ‚ÄúThe arcade learning environment: An evaluation platform for general agents‚Äù. In: Twenty-Fourth International Joint Conference on Artificial Intelligence. 2015.
[Bro+16] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. ‚ÄúOpenAI Gym‚Äù. In: arXiv preprint arXiv:1606.01540 (2016).
[Dua+16] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. ‚ÄúBenchmarking Deep Reinforcement Learning for Continuous Control‚Äù. In: arXiv preprint arXiv:1604.06778 (2016).
[Hee+17] N. Heess, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez, Z. Wang, A. Eslami, M. Riedmiller, et al. ‚ÄúEmergence of Locomotion Behaviours in Rich Environments‚Äù. In: arXiv preprint arXiv:1707.02286 (2017).
[KL02] S. Kakade and J. Langford. ‚ÄúApproximately optimal approximate reinforcement learning‚Äù. In: ICML. Vol. 2. 2002, pp. 267‚Äì274.
[KB14] D. Kingma and J. Ba. ‚ÄúAdam: A method for stochastic optimization‚Äù. In: arXiv preprint arXiv:1412.6980 (2014).
[Mni+15] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. ‚ÄúHuman-level control through deep reinforcement learning‚Äù. In: Nature 518.7540 (2015), pp. 529‚Äì533.
[Mni+16] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. ‚ÄúAsynchronous methods for deep reinforcement learning‚Äù. In: arXiv preprint arXiv:1602.01783 (2016).
[Sch+15a] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. ‚ÄúHigh-dimensional continuous control using generalized advantage estimation‚Äù. In: arXiv preprint arXiv:1506.02438 (2015).
[Sch+15b] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. ‚ÄúTrust region policy optimization‚Äù. In: CoRR, abs/1502.05477 (2015).
[SL06] I. Szita and A. L¬®orincz. ‚ÄúLearning Tetris using the noisy cross-entropy method‚Äù. In: Neural computation 18.12 (2006), pp. 2936‚Äì2941.
[TET12] E. Todorov, T. Erez, and Y. Tassa. ‚ÄúMuJoCo: A physics engine for model-based control‚Äù. In: Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on. IEEE. 2012, pp. 5026‚Äì5033.
[Wan+16] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas. ‚ÄúSample Efficient Actor-Critic with Experience Replay‚Äù. In: arXiv preprint arXiv:1611.01224 (2016).
[Wil92] R. J. Williams. ‚ÄúSimple statistical gradient-following algorithms for connectionist reinforcement learning‚Äù. In: Machine learning 8.3-4 (1992), pp. 229‚Äì256.</pre>

## A Hyperparameters


Hyperparameter | Value
--- | ---
Horizon (T) | 2048
Adam stepsize | 3 √ó 10‚àí4
Num. epochs | 10
Minibatch size | 64
Discount (Œ≥) | 0.99
GAE parameter (Œª) | 0.95

Table 3: PPO hyperparameters used for the Mujoco 1 million timestep benchmark.


Hyperparameter | Value
--- | ---
Horizon (T) | 512
Adam stepsize | ‚àó
Num. epochs | 15
Minibatch size | 4096
Discount (Œ≥) | 0.99
GAE parameter (Œª) | 0.95
Number of actors | 32 (locomotion), 128 (flagrun)
Log stdev. of action distribution | LinearAnneal(‚àí0.7, ‚àí1.6)

Table 4: PPO hyperparameters used for the Roboschool experiments. Adam stepsize was adjusted based on the target value of the KL divergence.


Hyperparameter | Value
--- | ---
Horizon (T) | 128
Adam stepsize | 2.5 √ó 10‚àí4 √ó Œ±
Num. epochs | 3
Minibatch size | 32 √ó 8
Discount (Œ≥) | 0.99
GAE parameter (Œª) | 0.95
Number of actors | 8
Clipping parameter  | 0.1 √ó Œ±
VF coeff. c1 (9) | 1
Entropy coeff. c2 (9) | 0.01

Table 5: PPO hyperparameters used in Atari experiments. Œ± is linearly annealed from 1 to 0 over the course of learning.


## B Teljes√≠tm√©ny tov√°bbi Atari j√°t√©kokon

Itt bemutatjuk a PPO √©s az A2C √∂sszehasonl√≠t√°s√°t egy nagyobb, 49 Atari-j√°t√©kb√≥l √°ll√≥ gy≈±jtem√©nyben. A 6. √°bra a h√°rom v√©letlenszer≈± mag tanul√°si g√∂rb√©it mutatja, m√≠g a 6. t√°bl√°zat az √°tlagos teljes√≠tm√©nyt mutatja.


6. √°bra: A PPO √©s az A2C √∂sszehasonl√≠t√°sa mind a 49 ATARI j√°t√©kon, amelyek az OpenAI Gymben szerepeltek a megjelen√©s id≈ëpontj√°ban.

| | A2C | ACER | PPO |
| --- | --- | --- | --- |
| Alien | 1141.7 | 1655.4 | 1850.3 |
| Amidar | 380.8 | 827.6 | 674.6 |
| Assault | 1562.9 | 4653.8 | 4971.9 |
| Asterix | 3176.3 | 6801.2 | 4532.5 |
| Asteroids | 1653.3 | 2389.3 | 2097.5 |
| Atlantis | 729265.3 | 1841376.0 | 2311815.0 |
| BankHeist | 1095.3 | 1177.5 | 1280.6 |
| BattleZone | 3080.0 | 8983.3 | 17366.7 |
| BeamRider | 3031.7 | 3863.3 | 1590.0 |
| Bowling | 30.1 | 33.3 | 40.1 |
| Boxing | 17.7 | 98.9 | 94.6 |
| Breakout | 303.0 | 456.4 | 274.8 |
| Centipede | 3496.5 | 8904.8 | 4386.4 |
| ChopperCommand | 1171.7 | 5287.7 | 3516.3 |
| CrazyClimber | 107770.0 | 132461.0 | 110202.0 |
| DemonAttack | 6639.1 | 38808.3 | 11378.4 |
| DoubleDunk | -16.2 | -13.2 | -14.9 |
| Enduro | 0.0 | 0.0 | 758.3 |
| FishingDerby | 20.6 | 34.7 | 17.8 |
| Freeway | 0.0 | 0.0 | 32.5 |
| Frostbite | 261.8 | 285.6 | 314.2 |
| Gopher | 1500.9 | 37802.3 | 2932.9 |
| Gravitar | 194.0 | 225.3 | 737.2 |
| IceHockey | -6.4 | -5.9 | -4.2 |
| Jamesbond | 52.3 | 261.8 | 560.7 |
| Kangaroo | 45.3 | 50.0 | 9928.7 |
| Krull | 8367.4 | 7268.4 | 7942.3 |
| KungFuMaster | 24900.3 | 27599.3 | 23310.3 |
| MontezumaRevenge | 0.0 | 0.3 | 42.0 |
| MsPacman | 1626.9 | 2718.5 | 2096.5 |
| NameThisGame | 5961.2 | 8488.0 | 6254.9 |
| Pitfall | -55.0 | -16.9 | -32.9 |
| Pong | 19.7 | 20.7 | 20.7 |
| PrivateEye | 91.3 | 182.0 | 69.5 |
| Qbert | 10065.7 | 15316.6 | 14293.3 |
| Riverraid | 7653.5 | 9125.1 | 8393.6 |
| RoadRunner | 32810.0 | 35466.0 | 25076.0 |
| Robotank | 2.2 | 2.5 | 5.5 |
| Seaquest | 1714.3 | 1739.5 | 1204.5 |
| SpaceInvaders | 744.5 | 1213.9 | 942.5 |
| StarGunner | 26204.0 | 49817.7 | 32689.0 |
| Tennis | -22.2 | -17.6 | -14.8 |
| TimePilot | 2898.0 | 4175.7 | 4342.0 |
| Tutankham | 206.8 | 280.8 | 254.4 |
| UpNDown | 17369.8 | 145051.4 | 95445.0 |
| Venture | 0.0 | 0.0 | 0.0 |
| VideoPinball | 19735.9 | 156225.6 | 37389.0 |
| WizardOfWor | 859.0 | 2308.3 | 4185.3 |
| Zaxxon | 16.3 | 29.0 | 5008.7 |

6. t√°bl√°zat: A PPO √©s A2C √°tlagos v√©geredm√©nyei (legut√≥bbi 100 epiz√≥d) Atari j√°t√©kokon 40 milli√≥ j√°t√©kkocka (10 milli√≥) ut√°n
id≈ël√©p√©sek).
